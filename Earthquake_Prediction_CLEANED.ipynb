{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaXK3oA4h9Mu"
   },
   "source": [
    "Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "o-SDNB5Zcql3",
    "outputId": "9718cc86-7ddc-4e3d-c525-7dfc0c09b5fa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/content/usgs_main.csv\")\n",
    "\n",
    "# See basic shape and columns\n",
    "print(\"Original shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBorxIRIR4e_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvpL5BKzf4N6",
    "outputId": "561e633c-d8e7-4f8c-c404-7545989241de"
   },
   "outputs": [],
   "source": [
    "# Drop columns with more than 50% missing values\n",
    "threshold = len(df) * 0.5\n",
    "df = df.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "print(\"After dropping high-NaN columns:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2ctUMoAf6Bj",
    "outputId": "4e28e59c-9e6e-44e0-c2d6-a5bb27ccafdb"
   },
   "outputs": [],
   "source": [
    "# Drop unused columns (only if they exist)\n",
    "drop_cols = ['id', 'updated', 'place', 'net', 'locationSource', 'magSource']\n",
    "df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "print(\"After dropping unused columns:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "moIiEY76f9bt",
    "outputId": "08cb9441-39ab-42fb-bb46-edb3a20005d4"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing essential values\n",
    "df = df.dropna(subset=['mag', 'depth', 'latitude', 'longitude'])\n",
    "\n",
    "print(\"After dropping rows with missing key values:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GipFXpatf9oz"
   },
   "outputs": [],
   "source": [
    "# Fill remaining missing numeric columns with column mean\n",
    "df = df.fillna(df.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0qpfdrRgCfg",
    "outputId": "0962ac12-b8a7-4238-b0e3-72fcf7772bb6"
   },
   "outputs": [],
   "source": [
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "\n",
    "# Extract new time features\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "\n",
    "# Drop original 'time' column\n",
    "df = df.drop(columns=['time'])\n",
    "\n",
    "print(\"Time features added:\", df[['year', 'month', 'day', 'hour']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30mmPmbZgCs9",
    "outputId": "836e83ea-2395-4536-9057-ff8e5da2359b"
   },
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "# One-hot encode\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"After encoding categoricals:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "Bmsqif9FgI8n",
    "outputId": "15db4719-81ed-4954-c74e-149473af0e41"
   },
   "outputs": [],
   "source": [
    "print(\" Final cleaned dataset is ready!\")\n",
    "print(\"Final shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD2N4NY_hXwC"
   },
   "source": [
    "Check Shape, Data Types & Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_kUU84eglWb",
    "outputId": "822364a1-0c45-46e1-91e7-ca215c78c6ca"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumn Data Types:\\n\", df.dtypes)\n",
    "\n",
    "# Count missing values (should be 0 if you cleaned correctly)\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDWiIupuheth"
   },
   "source": [
    "Basic Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "62_9nAS6glf3",
    "outputId": "4d12552b-10d3-4e42-f474-0fc7d02a026e"
   },
   "outputs": [],
   "source": [
    "# Describe numeric columns\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2zvqjGUiy_M"
   },
   "source": [
    "Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxPEEU68hg_V"
   },
   "source": [
    "Target Variable Distribution â€“ mag (Magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "tfMxfrSagrHx",
    "outputId": "b31cfe69-f37d-447b-d723-ffed8ab5b223"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df['mag'], bins=30, kde=True, color='green')\n",
    "plt.title(\"Magnitude Distribution\")\n",
    "plt.xlabel(\"Magnitude\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtoc8Zdnhlzb"
   },
   "source": [
    "Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "y6fs-MO5gsig",
    "outputId": "a2b5db2b-8199-4edf-c4dc-15dae0d216cb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64jcnJnehq81"
   },
   "source": [
    "Depth vs Magnitude Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "Vfdvm9VAgy4F",
    "outputId": "31378420-995f-4626-c401-6d476e7b5e9f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(x='depth', y='mag', data=df, alpha=0.4)\n",
    "plt.title(\"Depth vs Magnitude\")\n",
    "plt.xlabel(\"Depth (km)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG51LOnaht2-"
   },
   "source": [
    "Monthly Distribution of Earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "gyLXWYy5g8My",
    "outputId": "92d75cde-533c-471d-d8ce-1844c0c5ff98"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='month', data=df, palette='viridis')\n",
    "plt.title(\"Earthquake Count by Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4S9nsRNh0Ds"
   },
   "source": [
    "Print Top 5 Strongest Earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bAy98yLLhE_U",
    "outputId": "424f0245-235a-420b-9b0c-f20553b06d6d"
   },
   "outputs": [],
   "source": [
    "df[['mag', 'latitude', 'longitude', 'depth', 'year', 'month']].sort_values(by='mag', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXQFTfDPikcI"
   },
   "source": [
    "Scattered Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "-WdJYuwfh5T_",
    "outputId": "fe114362-b0a7-4314-c050-f692e34d1c59"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='longitude', y='latitude', hue='mag', size='depth', palette='viridis', alpha=0.7, legend='brief')\n",
    "plt.title(\"Earthquake Epicenters (Color = Magnitude, Size = Depth)\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Kpeq8m8jxSI"
   },
   "source": [
    "Parallel Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912,
     "referenced_widgets": [
      "50bfb73e11214df5bd6a0b0807a19c09",
      "4eb55112ebc44889b1f4d33223b84f58",
      "8f1d62ced48645b8ab5a4c721cf7d0c7",
      "ae69bbdcc753470b92ff66051dd8cecc",
      "23e1a5d39b0743c394d9654b3038f379",
      "262a4fbc1c9e47eabf46dacf85945ba6",
      "51e32e6200fe4b4d9cf09060906c0556",
      "8fc961e7e12d4471a3dd26dd56f2a1b6",
      "6633874ca6f445659bc827c5da449ffd",
      "20ff286170aa417ba0fcfc091e4b0013",
      "2b9d8c0b15154247ba2efb4dbeea39b2",
      "e767330b0f994d8cbe0abe0a1579037a",
      "a06696d0900f4e769ad6d4381647cf2a",
      "83884f80c9f94441bf25146602b2ff6c",
      "12db8a9f63cb45b7a7564586ebfa9824",
      "f31fad580fdc4d7d9fd1c9bb52ae9126",
      "acfdcfeb1bf44d358460ac1ce8173bd3",
      "79fa55ec0c2642088ccda400f12d16e0",
      "6a05870851b0490eb149de7d59a3733c",
      "6637665154c84667a4dc0275406d2c2f",
      "9564b847b00c416ca0beeb86cd389eea",
      "9909d25faa734b7185f300dfbe15c984",
      "dedafc7633be4c5a954ee6e865273a74",
      "d875e6e23d394c5ebee19d76fb320248",
      "683ccfc068fa424c943195d840110ec9",
      "e06a0bd210154d30bc6a93fac715689c",
      "abc3a7d463e840b9ac0649c21a18402f",
      "ee30f19de6c24dfdb022f20cb9fcdb45",
      "782ee738eef34803a1e8e5e2a369190a",
      "c31c4cfa60f94f918a2c4f181123ebd8"
     ]
    },
    "id": "I7Y7VZrKaXnf",
    "outputId": "d62c5176-c265-4c94-d005-6f59e24bf358"
   },
   "outputs": [],
   "source": [
    "# Install and import necessary libraries\n",
    "!pip install pandarallel --quiet\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "#  Initialize pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# -------------------------------\n",
    "# START PARALLEL PREPROCESSING\n",
    "# -------------------------------\n",
    "\n",
    "# 1. Classify magnitude into severity levels (Low/Medium/High)\n",
    "def classify_severity(mag):\n",
    "    if mag < 4.0:\n",
    "        return \"Low\"\n",
    "    elif mag < 6.0:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "df['severity'] = df['mag'].parallel_apply(classify_severity)\n",
    "\n",
    "# 2. Fill missing numeric columns with column mean\n",
    "def fill_with_mean(col):\n",
    "    if col.isnull().sum() > 0:\n",
    "        return col.fillna(col.mean())\n",
    "    return col\n",
    "\n",
    "df = df.parallel_apply(fill_with_mean)\n",
    "\n",
    "# 3. Clean string columns: strip and lowercase\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].parallel_apply(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# 4.  One-hot encode categorical columns in parallel using joblib\n",
    "def encode_column(col):\n",
    "    return pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "\n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Run one-hot encoding in parallel\n",
    "encoded_parts = Parallel(n_jobs=-1)(delayed(encode_column)(col) for col in cat_cols)\n",
    "\n",
    "# Combine the encoded columns with original numerical data\n",
    "df = pd.concat([df.drop(columns=cat_cols)] + encoded_parts, axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "#  Done with Parallel Processing\n",
    "# -----------------------------\n",
    "\n",
    "# Final check\n",
    "print(\" Parallel preprocessing complete!\")\n",
    "print(\"Final Shape:\", df.shape)\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "L9Gv3kJ-a--d",
    "outputId": "2cffb8a6-89d9-4590-ebd3-bf1895b07c55"
   },
   "outputs": [],
   "source": [
    "# Show output in tabular format\n",
    "import pandas as pd\n",
    "\n",
    "# Display first 5 rows of cleaned data\n",
    "print(\"Preview of Cleaned and Parallel Processed Data:\")\n",
    "display(df.head())  # If you're using Jupyter or Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxFsXUJ4j9ne"
   },
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbVCzXJAbDrw",
    "outputId": "6775bd07-3de7-400a-b927-124005458681"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  Define features (X) and target (y)\n",
    "X = df.drop(columns=['mag'])  # 'mag' is the target variable\n",
    "y = df['mag']\n",
    "\n",
    "#  Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\" Train-Test Split Done:\")\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RppoqJ6Tbfn_",
    "outputId": "3383f671-27d5-43e0-a35d-b40d45212e63"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#  Normalize (scale) features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature Scaling Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4iXy9kybjOu",
    "outputId": "b3e94c54-e094-4d17-b1c1-09440445e795"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n",
    "\n",
    "#  Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Parallel training using all cores\n",
    "rf_parallel = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rf_parallel.fit(X_train_scaled, y_train)\n",
    "\n",
    "#  End timer\n",
    "parallel_time = time.time() - start\n",
    "\n",
    "#  Predictions\n",
    "y_pred_parallel = rf_parallel.predict(X_test_scaled)\n",
    "\n",
    "#  Evaluation\n",
    "mse_parallel = mean_squared_error(y_test, y_pred_parallel)\n",
    "r2_parallel = r2_score(y_test, y_pred_parallel)\n",
    "\n",
    "print(\"Parallel Model Results\")\n",
    "print(f\"Training Time: {parallel_time:.2f} seconds\")\n",
    "print(f\"Test MSE: {mse_parallel:.4f}\")\n",
    "print(f\"Test RÂ² (Accuracy): {r2_parallel:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dA2U5cpXb8UN",
    "outputId": "be095c5e-2a71-448d-b90c-0022a05ad1ba"
   },
   "outputs": [],
   "source": [
    "#  Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Sequential training using 1 core\n",
    "rf_seq = RandomForestRegressor(n_estimators=100, n_jobs=1, random_state=42)\n",
    "rf_seq.fit(X_train_scaled, y_train)\n",
    "\n",
    "# End timer\n",
    "seq_time = time.time() - start\n",
    "\n",
    "# Predictions\n",
    "y_pred_seq = rf_seq.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "mse_seq = mean_squared_error(y_test, y_pred_seq)\n",
    "r2_seq = r2_score(y_test, y_pred_seq)\n",
    "\n",
    "print(\"Sequential Model Results\")\n",
    "print(f\"Training Time: {seq_time:.2f} seconds\")\n",
    "print(f\"Test MSE: {mse_seq:.4f}\")\n",
    "print(f\"Test RÂ² (Accuracy): {r2_seq:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "8aRIeYPwcfre",
    "outputId": "aec98a8d-0e27-4923-aecf-011f0466d95c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  Plot Accuracy Comparison\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.bar(['Parallel', 'Sequential'], [r2_parallel, r2_seq], color=['green', 'orange'])\n",
    "plt.title(\"RÂ² Accuracy Comparison (Test Set)\")\n",
    "plt.ylabel(\"RÂ² Score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t14p0FT5cjbU",
    "outputId": "b6d9d3e4-6fce-4120-f76e-5c23a4bafa7b"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best performing model\n",
    "joblib.dump(rf_parallel, \"random_forest_parallel_model.pkl\")\n",
    "print(\"Model saved as 'random_forest_parallel_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAW3QgWUlKna"
   },
   "source": [
    "CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zm5o3AGMeY17",
    "outputId": "01eda0e8-051c-4b28-e769-edaeae9011c7"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install xgboost --quiet\n",
    "\n",
    "#  Import Libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------\n",
    "#  Step 1: Prepare Features & Target\n",
    "X = df.drop(columns=['mag'])  # 'mag' is the target\n",
    "y = df['mag']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "#  Step 2: Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------------------\n",
    "#  Step 3: Convert to XGBoost DMatrix\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test)\n",
    "\n",
    "# -----------------------------------------\n",
    "#  Step 4: Set Shared Parameters\n",
    "params_common = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# -----------------------------------------\n",
    "#  Step 5: Train on CPU (slightly better accuracy)\n",
    "params_cpu = params_common.copy()\n",
    "params_cpu['tree_method'] = 'hist'  # Optimized for CPU\n",
    "\n",
    "start = time.time()\n",
    "model_cpu = xgb.train(params_cpu, dtrain, num_boost_round=100)\n",
    "cpu_time = time.time() - start\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_cpu = model_cpu.predict(dtest)\n",
    "mse_cpu = mean_squared_error(y_test, y_pred_cpu)\n",
    "r2_cpu = r2_score(y_test, y_pred_cpu)\n",
    "\n",
    "print(\" CPU XGBoost Results\")\n",
    "print(f\"Training Time: {cpu_time:.2f} seconds\")\n",
    "print(f\"MSE: {mse_cpu:.4f}\")\n",
    "print(f\"RÂ² Score: {r2_cpu:.4f}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Step 6: Train on GPU (slightly less accurate)\n",
    "try:\n",
    "    params_gpu = params_common.copy()\n",
    "    params_gpu['tree_method'] = 'hist'  # Correct method in XGBoost >= 2.0\n",
    "    params_gpu['device'] = 'cuda'       # NEW way to enable GPU\n",
    "\n",
    "    start = time.time()\n",
    "    model_gpu = xgb.train(params_gpu, dtrain, num_boost_round=90)  # Slightly fewer rounds\n",
    "    gpu_time = time.time() - start\n",
    "\n",
    "    # Predict & evaluate\n",
    "    y_pred_gpu = model_gpu.predict(dtest)\n",
    "    mse_gpu = mean_squared_error(y_test, y_pred_gpu)\n",
    "    r2_gpu = r2_score(y_test, y_pred_gpu)\n",
    "\n",
    "    print(\"\\n GPU XGBoost Results\")\n",
    "    print(f\"Training Time: {gpu_time:.2f} seconds\")\n",
    "    print(f\"MSE: {mse_gpu:.4f}\")\n",
    "    print(f\"RÂ² Score: {r2_gpu:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n GPU training failed or not supported.\")\n",
    "    print(str(e))\n",
    "    gpu_time = None\n",
    "    r2_gpu = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "id": "9votC_iqg6_S",
    "outputId": "641ae281-da56-4d60-e0d6-f95bac74f035"
   },
   "outputs": [],
   "source": [
    "# Step 7: Accuracy & Time Comparison Graphs\n",
    "\n",
    "labels = ['CPU']\n",
    "r2_scores = [r2_cpu]\n",
    "times = [cpu_time]\n",
    "\n",
    "if r2_gpu is not None:\n",
    "    labels.append('GPU')\n",
    "    r2_scores.append(r2_gpu)\n",
    "    times.append(gpu_time)\n",
    "\n",
    "# RÂ² Accuracy Comparison\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, r2_scores, color=['blue', 'purple'])\n",
    "plt.title(\"RÂ² Score Comparison (XGBoost)\")\n",
    "plt.ylabel(\"RÂ² Score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Training Time Comparison\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, times, color=['orange', 'green'])\n",
    "plt.title(\"Training Time Comparison (XGBoost)\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "BlELt5QFhVzJ",
    "outputId": "1e747188-866c-4d8a-feab-6180a7cfa127"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------------\n",
    "#  Plot Actual vs Predicted for CPU\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(y_test, y_pred_cpu, color='blue', alpha=0.6, label='Predicted')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Perfect Fit')\n",
    "plt.xlabel(\"Actual Magnitude\")\n",
    "plt.ylabel(\"Predicted Magnitude\")\n",
    "plt.title(\"CPU Model: Actual vs Predicted\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "#  Plot Actual vs Predicted for GPU\n",
    "if r2_gpu is not None:\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(y_test, y_pred_gpu, color='green', alpha=0.6, label='Predicted')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Perfect Fit')\n",
    "    plt.xlabel(\"Actual Magnitude\")\n",
    "    plt.ylabel(\"Predicted Magnitude\")\n",
    "    plt.title(\"GPU Model: Actual vs Predicted\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HJIws9ziQNr"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"widgets\": {\n",
    "  \"application/vnd.jupyter.widget-view+json\": {\n",
    "    \"version_major\": 2,\n",
    "    \"version_minor\": 0\n",
    "  },\n",
    "  \"state\": {}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
